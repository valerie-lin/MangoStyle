{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNeXT_v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnIMSPv-WrEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gdown --id '1WTI8HzfuqWyH3IIeLG955LUgmnjISzgS' --output Dev.tar.xz\n",
        "!tar -xvf  'Dev.tar.xz' \n",
        "\n",
        "!gdown --id '1pL7uF5Ej-IWYwdcXG37JSQFTUnFWgE07' --output train.tar.xz\n",
        "!tar -xvf  'train.tar.xz'\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA2U3JBQWxU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torchvision.models as models\n",
        "import torch\n",
        "from torch.optim import lr_scheduler\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import argparse\n",
        "import os\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import time\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eDdTQfGXtBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_selection import SelectPercentile, chi2\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import recall_score\n",
        "import pickle\n",
        "\n",
        "import sklearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD7PLFwRgKWu",
        "colab_type": "code",
        "outputId": "48cf8dff-597f-4c42-aeb5-da3c284af5e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dev_1.csv     Dev.tar.xz       sample_data  train_selected\n",
            "dev_selected  model_res18.pkl  train_1.csv  train.tar.xz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQjh62j-sx9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transform = transforms.Compose([\n",
        "                transforms.ToPILImage(),\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.RandomRotation(degrees=15),\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                        std =[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(), \n",
        "    transforms.Resize((224, 224)),                                   \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                        std =[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "class MangoDataset(Dataset):\n",
        "    def __init__(self, txtName, folderName, transform=None):\n",
        "        imgs = []\n",
        "        with open(txtName, 'r') as txtFile:\n",
        "            lines = txtFile.readlines()\n",
        "            for line in lines:\n",
        "                line = line.strip('\\n')\n",
        "                img, label = line.split(',')\n",
        "                if(len(label)!=1):\n",
        "                    #ignore header\n",
        "                    continue\n",
        "                imgs.append((img, ord(label)-ord('A')))\n",
        "        \n",
        "        self.imgs = imgs\n",
        "        self.transform = transform\n",
        "        #print(len(imgs), folderName)\n",
        "        self.folder = Path(folderName)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        imgName, label = self.imgs[index]\n",
        "        imgPath = self.folder / imgName\n",
        "        img = cv2.imread(str(imgPath))\n",
        "        #print(img.shape)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        \n",
        "        return img, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9u0pIg7WvaOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 25\n",
        "\n",
        "train_data = MangoDataset(txtName='train_1.csv', folderName='train_selected',transform=train_transform)\n",
        "data_loader = DataLoader(train_data, batch_size =batch_size , shuffle=True)\n",
        "\n",
        "dev_data = MangoDataset(txtName='dev_1.csv', folderName='dev_selected',transform=test_transform)\n",
        "val_loader = DataLoader(dev_data, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9rAy7pYKYum",
        "colab_type": "code",
        "outputId": "94a1c4ea-b34a-4e3e-cfc0-ea7a174c3800",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(len(data_loader))\n",
        "print(len(val_loader))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "224\n",
            "32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGOE9BZJWzux",
        "colab_type": "code",
        "outputId": "c39a17f0-f3f2-4f91-db72-4e618d0460b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print('The torch version is {}.'.format(torch.__version__))\n",
        "print('The scikit-learn version is {}.'.format(sklearn.__version__))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('device', device)\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, inchannel, outchannel, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.left = nn.Sequential(\n",
        "            nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(outchannel),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(outchannel)\n",
        "        )\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or inchannel != outchannel:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(outchannel)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.left(x)\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, ResidualBlock, num_classes=3):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.inchannel = 64\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.layer1 = self.make_layer(ResidualBlock, 64,  2, stride=1)\n",
        "        self.layer2 = self.make_layer(ResidualBlock, 128, 2, stride=2)\n",
        "        self.layer3 = self.make_layer(ResidualBlock, 256, 2, stride=2)\n",
        "        self.layer4 = self.make_layer(ResidualBlock, 512, 2, stride=2)\n",
        "        self.fc = nn.Linear(512* 7 * 7, num_classes)\n",
        "\n",
        "    def make_layer(self, block, channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)   #strides=[1,1]\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.inchannel, channels, stride))\n",
        "            self.inchannel = channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(ResidualBlock)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The torch version is 1.5.0+cu101.\n",
            "The scikit-learn version is 0.22.2.post1.\n",
            "device cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBwSlt2LYea_",
        "colab_type": "text"
      },
      "source": [
        "## Training:\n",
        "batch_size = 20, epoch = 40;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyMsgJYOyBQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = ResNet18().to(device)  # use GPU\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "#optimizer =  torch.optim.Adam(model.parameters(), lr=1e-5) #torch.optim.SGD(model.parameters(), lr=0.001, momentum = 0.9) \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr =0.001, momentum=0.9, weight_decay=5e-4)\n",
        "#torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=False, threshold=0.00001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)\n",
        "#scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
        "\n",
        "num_epoch = 40\n",
        "\n",
        "\n",
        "v_batch = len(val_loader)\n",
        "loss_values = []\n",
        "valid_loss = []\n",
        "for epoch in range(num_epoch):\n",
        "    epoch_start_time = time.time()\n",
        "    train_acc = 0.0\n",
        "    sum_loss = 0.0\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "\n",
        "    model.train() # train model\n",
        "    for i, data in enumerate(data_loader):\n",
        "        length = len(data_loader)\n",
        "        optimizer.zero_grad() \n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step() \n",
        "\n",
        "        sum_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels.data).cpu().sum()\n",
        "        loss_values.append(sum_loss / (i + 1))\n",
        "    print('TRAINING: [epoch:%d, iter:%d] Loss: %.03f | Acc: %.3f%% ' % (epoch + 1, (i + 1 + epoch * length), sum_loss / (i + 1), 100. * correct / total))\n",
        "    \n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_total = 0.0\n",
        "        val_correct = 0.0\n",
        "        val_loss = 0.0\n",
        "        for data in val_loader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += predicted.eq(labels.data).cpu().sum()\n",
        "            val_loss += loss.item()\n",
        "            acc = 100. * val_correct / val_total\n",
        "            valid_loss.append(val_loss)\n",
        "        print('VAL: [epoch:%d, time:%d] |Acc: %.3f%% ' % (epoch + 1, (time.time()-epoch_start_time), acc))\n",
        "        print(\"\\n\")\n",
        "    #scheduler.step(val_loss)\n",
        "\n",
        "torch.save(model, 'model_res18_2.pkl')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaWqXmfmPMUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(loss_values,color='green')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fnI68ttL1cD",
        "colab_type": "text"
      },
      "source": [
        "Note: Validation set acc is about 77-79%"
      ]
    }
  ]
}